{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning LLMs on a budget"
      ],
      "metadata": {
        "id": "gXRANzBgA-af"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhWo4Tv2A3y0",
        "outputId": "8d72d057-03b4-4c0d-81ef-be2e0afe1d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.108.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.9)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.10.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas datasets openai pydantic transformers trl huggingface_hub peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, EvaConfig\n",
        "from huggingface_hub import login\n",
        "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, EarlyStoppingCallback\n",
        "\n",
        "import datasets\n",
        "import torch"
      ],
      "metadata": {
        "id": "Kkz6DKE1A9yI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils\n",
        "\n",
        "Functions that will work on our dataset, determine available HW and demonstrate change in the model.\n",
        "\n",
        "We don't need to pay them much attention."
      ],
      "metadata": {
        "id": "Nfo7R0qDB_8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine available HW\n",
        "device = (\n",
        "      \"cuda\"\n",
        "      if torch.cuda.is_available()\n",
        "      else \"mps\"\n",
        "      if torch.backends.mps.is_available()\n",
        "      else \"cpu\"\n",
        "  )\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9_BTlLs5CCDJ",
        "outputId": "c7ba25e3-2c1d-4c4d-e2e8-702b2d37f025"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DEMO_TEMPLATE = \"\"\"\n",
        "===================================\n",
        "Query:\\n{question_content}\\n\n",
        "Expected response:\\n{answer_content}\\n\n",
        "Actual response:\\n{decoded_response}\\n\n",
        "\"\"\"\n",
        "\n",
        "def demo_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    samples: list[dict],\n",
        "    device: str,\n",
        ") -> None:\n",
        "    \"\"\"Demonstrate model behavior. Each sample is a list of message dictionaries,\n",
        "    alternating conversation roles.\n",
        "    \"\"\"\n",
        "\n",
        "    for sample in samples:\n",
        "        messages = sample[\"messages\"]\n",
        "        question = messages[0]\n",
        "        answer = messages[1]\n",
        "        formatted_question = tokenizer.apply_chat_template([question], tokenize=False)\n",
        "        tokenized_message = tokenizer(formatted_question, return_tensors=\"pt\").to(\n",
        "            device\n",
        "        )\n",
        "\n",
        "        question_len = tokenized_message[\"input_ids\"].shape[1]\n",
        "\n",
        "        # Reponse should not be much longer than what we expect,\n",
        "        # if it is something has gone wrong.\n",
        "        # We allow for pessimistic assumption that 1 char -> 1 token.\n",
        "        max_response_len = len(answer[\"content\"]) * 2\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **tokenized_message,\n",
        "            max_new_tokens=max_response_len,\n",
        "        )\n",
        "\n",
        "        # Remove tokens corresponding to question message\n",
        "        decoded_response = tokenizer.decode(\n",
        "            outputs[0][question_len:], skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            MODEL_DEMO_TEMPLATE.format(\n",
        "                question_content=question[\"content\"],\n",
        "                decoded_response=decoded_response,\n",
        "                answer_content=answer[\"content\"],\n",
        "            )\n",
        "        )\n"
      ],
      "metadata": {
        "id": "7lcuXM_8Cy88"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_split(\n",
        "    dataset: datasets.Dataset, n_demo_samples: int = 2\n",
        ") -> tuple[datasets.Dataset, datasets.Dataset, list[dict]]:\n",
        "    \"\"\"Create test/train/demo split for the dataset.\"\"\"\n",
        "\n",
        "    list_dataset = dataset.to_list()\n",
        "    dataset_split = len(list_dataset) // 5\n",
        "\n",
        "    demo, list_dataset = list_dataset[:n_demo_samples], list_dataset[n_demo_samples:]\n",
        "\n",
        "    train, test = list_dataset[dataset_split:], list_dataset[:dataset_split]\n",
        "    train = datasets.Dataset.from_list(train)\n",
        "    test = datasets.Dataset.from_list(test)\n",
        "\n",
        "    return train, test, demo"
      ],
      "metadata": {
        "id": "YKTQfu6iDPep"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to Hugging Face\n",
        "\n",
        "Some models and datasets require login to Hugging Face.\n",
        "If you do have a Hugging Face account, enter your token here."
      ],
      "metadata": {
        "id": "C1NKu8hWECg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "7c2002b11159411bbbd8f7a445c736df",
            "ce41d9ca2efc4eca8fbb0904cbf8bc1c",
            "e60c5a2949524d10a0bdd816085e0464",
            "79fe1f4c4e2f44fa80d0ad0ed7b84ea2",
            "8a487a84de624b8d9c90f60f3e7f1800",
            "f50b54f16ab44ad4a6f365c89d497f6e",
            "c41997d692cb4273bec9d6b93c0568f7",
            "a8f0e74fd0934abf8c47a34c5a927220",
            "ac1326fd64bd4be2813667f4a27aa188",
            "490366e03639420f8dd956044bcfd689",
            "c4ae17ef858944e192bff7f1c753eb70",
            "01c44be201884d28ab111203a0dac3be",
            "9c51d28d511a4b2c94ea88c0f43d606f",
            "a97f9c1c894a42119235af1edce7883e",
            "d80cc4545f7d43aab4d5a6ebaa809c84",
            "41ba6bce15cf4333b5bcaf6d72e62051",
            "29c63a2f79b9453c8d723a4e086b02c8"
          ]
        },
        "id": "izeQDCeYEAea",
        "outputId": "0aa59a77-da3e-4040-bb6e-da7df884c722"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c2002b11159411bbbd8f7a445c736df"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and tokenizer\n",
        "\n",
        "We'll use small 135 million parameter model from Hugging Face.\n",
        "The tokenizer already has a chat template, but in cases when it is absent, we can initialize a default.\n"
      ],
      "metadata": {
        "id": "awrKGrdIENVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        pretrained_model_name_or_path=base_model\n",
        "    ).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=base_model)\n",
        "\n",
        "# Replace chat template with a default.\n",
        "# This will break your model, if applied without care!\n",
        "if (not hasattr(tokenizer, \"chat_template\")\n",
        "    or tokenizer.chat_template is None\n",
        "):\n",
        "    model, tokenizer = setup_chat_format(model, tokenizer)\n"
      ],
      "metadata": {
        "id": "dQsPLiWTCN-_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "For dataset we will use https://huggingface.co/datasets/fedora-copr/packaging-qna which was generated using the routine in our script using Fedora packaging guidelines as a source.\n",
        "\n",
        "Not available features. We'll be using `messages`, as they are already preformatted to work with trainer.\n",
        "\n",
        "When working wiht other datasets, this step has to be implemented separately. But it is simple enough transformation."
      ],
      "metadata": {
        "id": "zN3niW5uE4wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"fedora-copr/packaging-qna\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS61RHZnE7dU",
        "outputId": "2f212bc6-a03c-4ace-ab9b-4db9c5035c4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'answer', 'question_topic', 'source', 'document_topic', 'messages'],\n",
              "        num_rows: 81508\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Since the dataset has over 80 thousand elements, it would take too long to train on all of it. We'll instead use just 2000 elements. Our model won't be able to learn as much. But it's not going to take as much time."
      ],
      "metadata": {
        "id": "fMxtbtCfy5pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.Dataset.from_dict(dataset[\"train\"][:2000])\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO1s51umPa-O",
        "outputId": "d30d78ac-ae21-4995-ada4-2f83bbf12307"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'question_topic', 'source', 'document_topic', 'messages'],\n",
              "    num_rows: 2000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting dataset into training and evaluation subset is a necessity. Otherwise we wouldn't be able to estimate how well will our model perform with data it hasn't seen yet.\n",
        "\n",
        "We'll also choose couple of samples for demonstrating difference between model behavior before, and after training."
      ],
      "metadata": {
        "id": "lsTXkL3bzhjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, eval, demo_samples = create_dataset_split(dataset)"
      ],
      "metadata": {
        "id": "1-yJdiy1DJY-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each sample is a short conversation between model, in an `assistant` role, and `user`. In chatbot applications, these conversations can last much longer.\n",
        "\n",
        "For our purposes however, a single turn is sufficient."
      ],
      "metadata": {
        "id": "EA_DA3BY0AM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySIbnX4-GnFx",
        "outputId": "17f1ad19-953f-4de5-eca2-2cfc9b5452be"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'question': 'Where can large data files from the package be externally placed?',\n",
              "  'answer': 'If the package contains excessively large data files, they may be placed in a separate `-data` subpackage, as per normal Fedora guidelines.',\n",
              "  'question_topic': 'Packaging libraries',\n",
              "  'source': './packaging-committee/guidelines/modules/ROOT/pages/OCaml.adoc',\n",
              "  'document_topic': 'OCaml Packaging Guidelines',\n",
              "  'messages': [{'content': 'Where can large data files from the package be externally placed?',\n",
              "    'role': 'user'},\n",
              "   {'content': 'If the package contains excessively large data files, they may be placed in a separate `-data` subpackage, as per normal Fedora guidelines.',\n",
              "    'role': 'assistant'}]},\n",
              " {'question': 'What types of conflicts are absolutely unacceptable and under what conditions?',\n",
              "  'answer': 'Keep in mind that implicit conflicts are NEVER acceptable.',\n",
              "  'question_topic': 'Implicit Conflicts',\n",
              "  'source': './packaging-committee/guidelines/modules/ROOT/pages/Conflicts.adoc',\n",
              "  'document_topic': 'Conflicts Guidelines',\n",
              "  'messages': [{'content': 'What types of conflicts are absolutely unacceptable and under what conditions?',\n",
              "    'role': 'user'},\n",
              "   {'content': 'Keep in mind that implicit conflicts are NEVER acceptable.',\n",
              "    'role': 'assistant'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the `demo_model` function, we'll submit a small sample of questions to LLM before fine tuning and compare the output with responses in our dataset.\n"
      ],
      "metadata": {
        "id": "0XptrG5uHvmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_model(model, tokenizer, demo_samples, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lrXi1XCqQA",
        "outputId": "dc3d657a-f01f-43fb-dae0-6d38ad89ae0d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================\n",
            "Query:\n",
            "Where can large data files from the package be externally placed?\n",
            "\n",
            "Expected response:\n",
            "If the package contains excessively large data files, they may be placed in a separate `-data` subpackage, as per normal Fedora guidelines.\n",
            "\n",
            "Actual response:\n",
            "assistant\n",
            "SmolLM can be externally placed in various ways, depending on the specific requirements of your project. Here are some common methods:\n",
            "\n",
            "1. **Externalized Files**: You can upload your large data files to a cloud storage service like Google Drive, Dropbox, or OneDrive. These services allow you to upload your files to a centralized location, making it easy to access and share them with others.\n",
            "\n",
            "2. **Cloud Storage Services**: You can also use cloud storage services like Google Drive, Dropbox, or OneDrive to externalize your large data files. These services provide a centralized location for storing and sharing your files, making it easy to access and share them.\n",
            "\n",
            "3. **Data Repositories**: You can also use data repositories like GitHub, Bitbucket, or Dropbox to externalize your large data files. These repositories allow you to upload your files to a centralized location, making it easy to share and collaborate on your data.\n",
            "\n",
            "4. **Cloud Storage Services**: You can also use cloud storage services like Amazon S3, Google Cloud Storage, or Microsoft Azure Storage to externalize your large data files. These services provide a centralized location for storing and sharing your files, making it easy to access and share them.\n",
            "\n",
            "5. **Data Repositories**: You can also use data repositories like Google Drive, Dropbox, or One\n",
            "\n",
            "\n",
            "\n",
            "===================================\n",
            "Query:\n",
            "What types of conflicts are absolutely unacceptable and under what conditions?\n",
            "\n",
            "Expected response:\n",
            "Keep in mind that implicit conflicts are NEVER acceptable.\n",
            "\n",
            "Actual response:\n",
            "assistant\n",
            "The types of conflicts that are absolutely unacceptable and under what conditions are a matter of personal preference and cultural norms. However, here are some of the most common types of conflicts that are generally considered unacceptable and under what conditions:\n",
            "\n",
            "1. **Aggression**: Aggression is a fundamental human emotion that can be used to protect or defend oneself, others, or a group. However, it can also be used to harm or destroy innocent people.\n",
            "\n",
            "2. **Aggression against a group**: Aggression against a group can be a serious offense, such as murder\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning\n",
        "\n",
        "Instead of changing weights of the entire model, we will save up on memory, compute and time by using LoRa. This way we can train a much smaller model, and modify original models outputs with it.\n",
        "\n",
        "Eventually, we'll merge this model with the base, creating fine tuned version of the base model for our purposes."
      ],
      "metadata": {
        "id": "4WyfIRjHBz7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 1000\n",
        "output_dir = \"./sft_output\"\n",
        "finetune_name = \"FineTunedModel\"\n",
        "rank = 32\n"
      ],
      "metadata": {
        "id": "cKPDLOlEIP74"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRa is now a well developed methodology, with many options and augmentations.\n",
        "\n",
        "We'll use some of the more common ones. There are however many others that can be employed to great effect.\n",
        "\n",
        "This code uses DoRa, RSLoRa and EVA weight initialization.\n",
        "\n",
        "DoRA (Weight-Decomposed Low-Rank Adaptation) uses decomposition of base weights into magnitude and direction, and\n",
        "uses LoRa to update their directions. This approach has been show to consistently outperform base LoRa.\n",
        "\n",
        "https://huggingface.co/papers/2402.09353 \tarXiv:2402.09353\n",
        "\n",
        "\n",
        "RSLoRA (rank-stabilized LoRA) modifies the scaling factor of LoRa weights to square root of the rank. This is especially useful as rank of adapters increases.\n",
        "\n",
        "https://huggingface.co/papers/2312.03732 arXiv:2312.03732\n",
        "\n",
        "\n",
        "EVA (Explained Variance Adaptation) initializes weights of LoRa adapters based on training data, using singular value decomposition of activation vectors. This saves considerable time, as weights start in a state closer to our goal.\n",
        "\n",
        "https://huggingface.co/papers/2410.07170 arXiv:2410.07170\n",
        "\n",
        "\n",
        "To make further savings, we'll only train weights of linear modules from the base model. This cuts down the number of parameters in need of adjustment to small fraction of the original."
      ],
      "metadata": {
        "id": "YqAgGiC4Iw2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "      r=rank,\n",
        "      target_modules=\"all-linear\",  # Fine tune only linear modules\n",
        "      lora_alpha=rank,  # Set to rank for rslora\n",
        "      bias=\"none\",\n",
        "      use_rslora=True,\n",
        "      init_lora_weights=\"eva\",  # Initialize weights base on data arxiv:2410.07170\n",
        "      eva_config=EvaConfig(),\n",
        "      use_dora=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "-mOFJRUCBCMM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_config = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    max_steps=max_steps,  # Adjust based on dataset size and desired training duration\n",
        "    per_device_train_batch_size=4,  # Set according to your device memory capacity\n",
        "    learning_rate=1e-5,  # Common starting point for fine-tuning\n",
        "    logging_steps=10,  # Frequency of logging training metrics\n",
        "    save_steps=100,  # Frequency of saving model checkpoints\n",
        "    eval_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
        "    eval_steps=100,  # Frequency of evaluation\n",
        "    use_mps_device=(device == \"mps\"),  # Use MPS for mixed precision training\n",
        "    hub_model_id=finetune_name,  # Set a unique name for your model,\n",
        "    metric_for_best_model=\"eval_loss\",  # How we determine our improvement (used by callback)\n",
        "    load_best_model_at_end=True,  # Return the last best checkpoint\n",
        "    bf16=(device == \"cuda\"),  # Use bf16 only if you have CUDA device\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "qS9afJkVBsOg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train,\n",
        "    processing_class=tokenizer,\n",
        "    eval_dataset=eval,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(early_stopping_threshold=0.01)\n",
        "    ],  # Stop if we don't improve by at least X\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(f\"./{finetune_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511,
          "referenced_widgets": [
            "01ee2edd662f41dba4f99b3d61a8b6d3",
            "a09028e5b092426cb6af7689426b9dce",
            "bcfb897088b348fab3eb2d1c2e1cbe97",
            "6aee11c5cb854b3cb697a4de084b7198",
            "72c9fb68e5ef438cb7bbe5d35bdc3081",
            "1dc198e5c58b40d68618fc2e4e6d854c",
            "52acef4fb8494288bf9210f41dff8e10",
            "1f69f7b623ac44f9a55c44778888d4d6",
            "e980d686600d4998acd347ec9e79e3bd",
            "8027c395d84642ea8d6e784a10755286",
            "d5116677f006450f8a7f1a945178d7fc",
            "f8526ed70445418db0af0002e9e95d11",
            "8877cf1ba08144efbe099dabec61d524",
            "51e8420657b0409faf5b0063700e3c85",
            "1d80c78179af4734ac8c315826b657b2",
            "c159225fd42948909f9fe7ad1fb11a07",
            "8bde36c066e740229697ce9cf28ea007",
            "218455dd08464643996ec8937b9163e1",
            "3330d4d17a9c4d4294b2292b98b8b609",
            "19dddc0cb5e6425da2aa64f57a320396",
            "1120d2efca1e4a4fad0c0299bc4a6c40",
            "e2eca489300341b7b31168842531207d",
            "f014abc89fe845039b1c0b81e8321bf5",
            "81fa05a2f61b4626a2bd0000cc800e98",
            "9f97a2b7a37244e3b701536c0104d15d",
            "2979a35f82ab47f69c1e55c492d92ec8",
            "073075cf0578482f9d27d083e9574027",
            "64ae174060b94217b1e55f51e55e277d",
            "510892762e0a44fdbb609006f2212c77",
            "af374bd050e9490f8320aa4dbdcc2c55",
            "209fe9f09c1e4ca4a77b7e82f9d31e09",
            "60b61b1e61864a368461c4fd66cba72a",
            "8068593293ec4dd78de9d6f9e5f43c79",
            "d5740e2691964ab88e8c5f8cc5377c0a",
            "a7a17d069cd24a7f809128b8cec9a287",
            "9962c02a737d41b892429d88b9e90dc3",
            "372bfe2770204cc98cdde1168f0b6631",
            "35880ca9009c4c03af1cef5d6828f3b3",
            "9ddc2317427c4fc3a07c95839e78ff2b",
            "1e94a11cc60045c1a4c352645631f259",
            "809b5557d1e04b2c83e0984d4325d3e4",
            "679ef7e56fee40ca8fdc0a1d711b7b45",
            "8bf895ccec7542388cdf0c440ecc304b",
            "c2545a4570a14eeab5dca06276a713c8"
          ]
        },
        "id": "6f5hDxIWCoiJ",
        "outputId": "8616cf9d-39ff-477c-b51e-649ddec9f851"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:96: UserWarning: lora with eva initialization used with low_cpu_mem_usage=False. Setting low_cpu_mem_usage=True can improve the maximum batch size possible for eva initialization.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/1598 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01ee2edd662f41dba4f99b3d61a8b6d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/1598 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8526ed70445418db0af0002e9e95d11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f014abc89fe845039b1c0b81e8321bf5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5740e2691964ab88e8c5f8cc5377c0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='800' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 800/1000 10:18 < 02:34, 1.29 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Entropy</th>\n",
              "      <th>Num Tokens</th>\n",
              "      <th>Mean Token Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.169500</td>\n",
              "      <td>2.167062</td>\n",
              "      <td>2.217872</td>\n",
              "      <td>30690.000000</td>\n",
              "      <td>0.632381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.926800</td>\n",
              "      <td>2.014471</td>\n",
              "      <td>2.046737</td>\n",
              "      <td>62778.000000</td>\n",
              "      <td>0.641535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.989600</td>\n",
              "      <td>1.946958</td>\n",
              "      <td>1.956198</td>\n",
              "      <td>94773.000000</td>\n",
              "      <td>0.648312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.796100</td>\n",
              "      <td>1.908914</td>\n",
              "      <td>1.920975</td>\n",
              "      <td>126381.000000</td>\n",
              "      <td>0.653616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.762400</td>\n",
              "      <td>1.880907</td>\n",
              "      <td>1.889728</td>\n",
              "      <td>157601.000000</td>\n",
              "      <td>0.657177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.841500</td>\n",
              "      <td>1.864224</td>\n",
              "      <td>1.864713</td>\n",
              "      <td>189247.000000</td>\n",
              "      <td>0.660754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.837900</td>\n",
              "      <td>1.852224</td>\n",
              "      <td>1.837811</td>\n",
              "      <td>221024.000000</td>\n",
              "      <td>0.662814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.885700</td>\n",
              "      <td>1.843835</td>\n",
              "      <td>1.844848</td>\n",
              "      <td>252762.000000</td>\n",
              "      <td>0.664634</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_model(model, tokenizer, demo_samples, device=device)"
      ],
      "metadata": {
        "id": "ZAS57AVXJHqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52243bf-3cde-4f02-c2ba-2f8acc2095ed"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================\n",
            "Query:\n",
            "Where can large data files from the package be externally placed?\n",
            "\n",
            "Expected response:\n",
            "If the package contains excessively large data files, they may be placed in a separate `-data` subpackage, as per normal Fedora guidelines.\n",
            "\n",
            "Actual response:\n",
            "assistant\n",
            "Large data files from the package can be externally placed in the `+/usr/lib/python2.7/dist-packages/+` directory.\n",
            "\n",
            "\n",
            "\n",
            "===================================\n",
            "Query:\n",
            "What types of conflicts are absolutely unacceptable and under what conditions?\n",
            "\n",
            "Expected response:\n",
            "Keep in mind that implicit conflicts are NEVER acceptable.\n",
            "\n",
            "Actual response:\n",
            "assistant\n",
            "*If a package is not compiled with the `++` compiler flags,\n",
            "*it must be compiled with the `++` compiler flags.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ce0WkAC-1Iy-"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}
